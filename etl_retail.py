# Importation des biblioth√®ques n√©cessaires
import pandas as pd              # Pour manipuler les donn√©es sous forme de DataFrame
import numpy as np               # Pour les op√©rations math√©matiques et num√©riques
import logging                   # Pour journaliser les √©v√©nements du pipeline
from sklearn.preprocessing import MinMaxScaler  # Pour normaliser les donn√©es num√©riques

# Configuration du journal de log pour suivre les √©tapes et erreurs du pipeline
logging.basicConfig(filename='etl.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Fonction pour charger les donn√©es depuis un fichier CSV
def charger_donnees(fichier_csv):
    try:
        df = pd.read_csv(fichier_csv)  # Lecture du fichier CSV
        colonnes_attendues = {'ID_produit', 'Nom_produit', 'Quantite_vendue', 'Prix_unitaire', 'Date_vente'}
        
        # V√©rification que toutes les colonnes n√©cessaires sont pr√©sentes
        if not colonnes_attendues.issubset(df.columns):
            raise ValueError("Colonnes manquantes dans le fichier CSV.")
        
        logging.info("Chargement du fichier r√©ussi.")
        return df
    except FileNotFoundError:
        logging.error("Fichier non trouv√©.")
        raise
    except pd.errors.ParserError:
        logging.error("Erreur de parsing du fichier CSV.")
        raise
    except Exception as e:
        logging.error(f"Erreur lors du chargement des donn√©es : {str(e)}")
        raise

# Analyse exploratoire des donn√©es
def analyse_donnees(df):
    print("Aper√ßu des donn√©es :")
    print(df.head())  # Affiche les premi√®res lignes du dataset
    print("\nInfos g√©n√©rales :")
    print(df.info())  # Donne des infos sur les types et colonnes
    print("\nValeurs manquantes par colonne :")
    print(df.isnull().sum())  # Affiche le nombre de valeurs manquantes
    print("\nStatistiques descriptives :")
    print(df.describe())  # Statistiques de base sur les colonnes num√©riques

# Traitement des valeurs manquantes
def traitement_valeurs_manquantes(df):
    # Suppression des lignes o√π les colonnes critiques sont vides
    colonnes_critiques = ['ID_produit', 'Nom_produit', 'Quantite_vendue', 'Prix_unitaire', 'Date_vente']
    df.dropna(subset=colonnes_critiques, inplace=True)

    # Remplir les valeurs manquantes restantes
    df['Quantite_vendue'] = df['Quantite_vendue'].fillna(0)
    df['Prix_unitaire'] = df['Prix_unitaire'].fillna(df['Prix_unitaire'].median())

    logging.info("Traitement des valeurs manquantes termin√©.")
    return df

# Suppression des valeurs aberrantes
def gestion_valeurs_aberrantes(df):
    # Filtrage des lignes o√π la quantit√© ou le prix sont invalides (<= 0)
    df = df[(df['Quantite_vendue'] > 0) & (df['Prix_unitaire'] > 0)].copy()

    # Conversion des dates, suppression si conversion √©choue
    df['Date_vente'] = pd.to_datetime(df['Date_vente'], errors='coerce')
    df.dropna(subset=['Date_vente'], inplace=True)

    logging.info("Traitement des valeurs aberrantes termin√©.")
    return df

# Suppression des doublons exacts
def suppression_doublons(df):
    avant = len(df)
    df.drop_duplicates(inplace=True)
    apres = len(df)
    logging.info(f"{avant - apres} doublons supprim√©s.")
    return df

# Application de transformations enrichies
def transformations(df):
    # 1. Cr√©ation d'une colonne 'Montant_total'
    df['Montant_total'] = df['Quantite_vendue'] * df['Prix_unitaire']

    # 2. Normalisation du prix unitaire entre 0 et 1
    scaler = MinMaxScaler()
    df['Prix_unitaire_normalis√©'] = scaler.fit_transform(df[['Prix_unitaire']]).round(3)

    # 3. Extraction du mois et de l‚Äôann√©e de la date de vente
    df['Mois_vente'] = df['Date_vente'].dt.month
    df['Annee_vente'] = df['Date_vente'].dt.year

    # 4. Nettoyage des noms de produits (suppression espaces, mise en minuscules)
    df['Nom_produit'] = df['Nom_produit'].str.strip().str.lower()

    # 5. Classification du montant total en cat√©gories
    df['Categorie_montant'] = pd.cut(
        df['Montant_total'],
        bins=[-1, 50, 200, np.inf],
        labels=['faible', 'moyen', '√©lev√©']
    )

    # 6. Cat√©gorisation des produits selon le nom
    def categorie_produit(nom):
        if any(x in nom for x in ['chemise', 'pantalon', 'veste']):
            return 'habillement'
        elif any(x in nom for x in ['ordinateur', '√©cran']):
            return '√©lectronique'
        else:
            return 'autre'

    df['Categorie_produit'] = df['Nom_produit'].apply(categorie_produit)

    logging.info("Transformations enrichies appliqu√©es.")
    return df

# Validation des donn√©es nettoy√©es
def validation_crois√©e(df):
    # V√©rifie qu‚Äôil ne reste plus de valeurs manquantes
    assert df.isnull().sum().sum() == 0, "Des valeurs manquantes subsistent."
    # V√©rifie que toutes les quantit√©s sont strictement positives
    assert (df['Quantite_vendue'] > 0).all(), "Des quantit√©s invalides."
    # V√©rifie que la date est bien au format datetime
    assert pd.api.types.is_datetime64_any_dtype(df['Date_vente']), "Dates mal converties."
    
    logging.info("Validation crois√©e r√©ussie.")
    print("‚úÖ Validation crois√©e OK.")

# Sauvegarde du DataFrame nettoy√© dans un nouveau fichier CSV
def sauvegarde(df, chemin_sortie):
    df.to_csv(chemin_sortie, index=False, encoding='utf-8-sig')
    logging.info(f"Donn√©es sauvegard√©es dans {chemin_sortie}.")

# Pipeline complet : orchestration des √©tapes ETL
def pipeline_etl(fichier_entree, fichier_sortie):
    try:
        logging.info("D√©but du pipeline ETL.")
        df = charger_donnees(fichier_entree)         # √âtape 1 : Chargement
        analyse_donnees(df)                          # √âtape 2 : Exploration
        df = traitement_valeurs_manquantes(df)       # √âtape 3 : Valeurs manquantes
        df = gestion_valeurs_aberrantes(df)          # √âtape 4 : Valeurs aberrantes
        df = suppression_doublons(df)                # √âtape 5 : Doublons
        df = transformations(df)                     # √âtape 6 : Transformations
        validation_crois√©e(df)                       # √âtape 7 : Validation
        sauvegarde(df, fichier_sortie)               # √âtape 8 : Sauvegarde
        logging.info("Fin du pipeline ETL avec succ√®s.")
        print(f"üéâ ETL termin√© avec succ√®s. {len(df)} lignes sauvegard√©es dans '{fichier_sortie}'.")
    except Exception as e:
        logging.error(f"Erreur dans le pipeline ETL : {str(e)}")
        print("‚ùå Une erreur est survenue dans le pipeline ETL.")

# Lancement du pipeline si le script est ex√©cut√© directement
if __name__ == "__main__":
    pipeline_etl("ventes.csv", "vente_clean.csv")
